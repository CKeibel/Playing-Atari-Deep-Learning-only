{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# playing Atari with Deep Learning\n",
    "* ohne Frame Stack\n",
    "* kein Replay Memory und Epsilon Greedy\n",
    "\n",
    "(*) Bilder müssen auf kleineres Format reshaped werden als Original (210, 160, 3), da sonst zu Groß für GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Datensatz generieren per Zufallsspiel.\n",
    "Es werden nur Sequenzen gespeichert, die einen Reward zurückgeben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose game\n",
    "game = \"Pong-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import cv2\n",
    "\n",
    "class WarpFrame(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"\n",
    "        Warp frames to 84x84 as done in the Nature paper and later work.\n",
    "        :param env: (Gym Environment) the environment\n",
    "        \"\"\"\n",
    "        gym.ObservationWrapper.__init__(self, env)\n",
    "        self.width = 84\n",
    "        self.height = 84\n",
    "        self.observation_space = spaces.Box(low=0, high=255, shape=(self.height, self.width, 1),\n",
    "                                            dtype=env.observation_space.dtype)\n",
    "        \n",
    "    def observation(self, frame):\n",
    "        \"\"\"\n",
    "        returns the current observation from a frame\n",
    "        :param frame: ([int] or [float]) environment frame\n",
    "        :return: ([int] or [float]) the observation\n",
    "        \"\"\"\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (self.width, self.height), interpolation=cv2.INTER_AREA)\n",
    "        return frame[:, :, None]\n",
    "\n",
    "# zum erstellen des Spiels mit Wrappern\n",
    "def WarpFrameEnv(env_name):\n",
    "    env = gym.make(env_name)\n",
    "    env = WarpFrame(env)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n",
      "1000\n",
      "Dataset completed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "env = WarpFrameEnv(game)\n",
    "x_data = [] # Bilder (States)\n",
    "y_data = [] # Label (Aktionen)\n",
    "\n",
    "DATASET_SIZE = 1000\n",
    "saved_episodes = 0\n",
    "\n",
    "# Datensatz generieren\n",
    "the_end = False\n",
    "while not the_end:\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    episode_obs = []\n",
    "    episode_acts = []\n",
    "  \n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        episode_obs.append(state)\n",
    "        episode_acts.append(action)\n",
    "        state, reward, done, info = env.step(action)\n",
    "    \n",
    "        if reward < 0:\n",
    "            episode_acts = []\n",
    "            episode_obs = []\n",
    "        elif reward > 0:\n",
    "            x_data += episode_obs\n",
    "            y_data += episode_acts\n",
    "            episode_obs = []\n",
    "            episode_acts = []\n",
    "            saved_episodes += 1\n",
    "            print(saved_episodes)\n",
    "            if saved_episodes+1 > DATASET_SIZE:\n",
    "                print(\"Dataset completed\")\n",
    "                the_end = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teilen in Training und Test, shuffeln des Datensatzes\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "        x_data, y_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# konvertieren in richtiges Format\n",
    "x_train = np.array(x_train, dtype=np.float32)\n",
    "x_test = np.array(x_test, dtype=np.float32)\n",
    "y_train = np.array(y_train, dtype=np.float32)\n",
    "y_test = np.array(y_test, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Erstellen des künstlichen neuronalen Netzes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input und Output shape\n",
    "INPUT_SHAPE = np.shape(x_data)[1:]\n",
    "OUTPUT_SHAPE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "net_input = Input(shape=INPUT_SHAPE)\n",
    "x = Conv2D(filters=32, kernel_size=(8, 8), strides=(4, 4), padding=\"same\")(net_input)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(filters=64, kernel_size=(4, 4), strides=(2, 2), padding=\"same\")(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding=\"same\")(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(512)(x)\n",
    "x = Activation(\"relu\")(x)\n",
    "net_output = Dense(OUTPUT_SHAPE)(x)\n",
    "\n",
    "LOSS_FUNCTION = Huber()\n",
    "OPTIMIZER = Adam(lr=0.0005)\n",
    "\n",
    "model = Model(inputs=net_input, outputs=net_output)\n",
    "model.compile(loss=LOSS_FUNCTION, optimizer=OPTIMIZER, metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "62140/62140 [==============================] - 21s 331us/sample - loss: 1.2103 - mae: 1.6579\n",
      "Epoch 2/30\n",
      "62140/62140 [==============================] - 20s 321us/sample - loss: 1.0650 - mae: 1.5139\n",
      "Epoch 3/30\n",
      "62140/62140 [==============================] - 19s 311us/sample - loss: 1.0608 - mae: 1.5104\n",
      "Epoch 4/30\n",
      "62140/62140 [==============================] - 19s 306us/sample - loss: 1.0589 - mae: 1.5103\n",
      "Epoch 5/30\n",
      "62140/62140 [==============================] - 19s 302us/sample - loss: 1.0581 - mae: 1.5096\n",
      "Epoch 6/30\n",
      "62140/62140 [==============================] - 19s 304us/sample - loss: 1.0575 - mae: 1.5102\n",
      "Epoch 7/30\n",
      "62140/62140 [==============================] - 21s 334us/sample - loss: 1.0579 - mae: 1.5113\n",
      "Epoch 8/30\n",
      "62140/62140 [==============================] - 19s 304us/sample - loss: 1.0561 - mae: 1.5097\n",
      "Epoch 9/30\n",
      "62140/62140 [==============================] - 18s 297us/sample - loss: 1.0555 - mae: 1.5107\n",
      "Epoch 10/30\n",
      "62140/62140 [==============================] - 19s 301us/sample - loss: 1.0565 - mae: 1.5114\n",
      "Epoch 11/30\n",
      "62140/62140 [==============================] - 18s 296us/sample - loss: 1.0548 - mae: 1.5102\n",
      "Epoch 12/30\n",
      "62140/62140 [==============================] - 19s 303us/sample - loss: 1.0548 - mae: 1.5106\n",
      "Epoch 13/30\n",
      "62140/62140 [==============================] - 18s 296us/sample - loss: 1.0531 - mae: 1.5099\n",
      "Epoch 14/30\n",
      "62140/62140 [==============================] - 19s 300us/sample - loss: 1.0527 - mae: 1.5098\n",
      "Epoch 15/30\n",
      "62140/62140 [==============================] - 18s 294us/sample - loss: 1.0527 - mae: 1.5101\n",
      "Epoch 16/30\n",
      "62140/62140 [==============================] - 19s 298us/sample - loss: 1.0526 - mae: 1.5103\n",
      "Epoch 17/30\n",
      "62140/62140 [==============================] - 19s 298us/sample - loss: 1.0522 - mae: 1.5098\n",
      "Epoch 18/30\n",
      "62140/62140 [==============================] - 18s 294us/sample - loss: 1.0521 - mae: 1.5094\n",
      "Epoch 19/30\n",
      "62140/62140 [==============================] - 19s 299us/sample - loss: 1.0523 - mae: 1.5098\n",
      "Epoch 20/30\n",
      "62140/62140 [==============================] - 19s 302us/sample - loss: 1.0521 - mae: 1.5097\n",
      "Epoch 21/30\n",
      "62140/62140 [==============================] - 18s 293us/sample - loss: 1.0522 - mae: 1.5098\n",
      "Epoch 22/30\n",
      "62140/62140 [==============================] - 18s 294us/sample - loss: 1.0520 - mae: 1.5095\n",
      "Epoch 23/30\n",
      "62140/62140 [==============================] - 20s 323us/sample - loss: 1.0522 - mae: 1.5099\n",
      "Epoch 24/30\n",
      "62140/62140 [==============================] - 18s 288us/sample - loss: 1.0522 - mae: 1.5098\n",
      "Epoch 25/30\n",
      "62140/62140 [==============================] - 21s 343us/sample - loss: 1.0522 - mae: 1.5100\n",
      "Epoch 26/30\n",
      "62140/62140 [==============================] - 18s 295us/sample - loss: 1.0523 - mae: 1.5104\n",
      "Epoch 27/30\n",
      "62140/62140 [==============================] - 19s 299us/sample - loss: 1.0522 - mae: 1.5098\n",
      "Epoch 28/30\n",
      "62140/62140 [==============================] - 18s 297us/sample - loss: 1.0521 - mae: 1.5096\n",
      "Epoch 29/30\n",
      "62140/62140 [==============================] - 19s 299us/sample - loss: 1.0522 - mae: 1.5100\n",
      "Epoch 30/30\n",
      "62140/62140 [==============================] - 18s 295us/sample - loss: 1.0521 - mae: 1.5099\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 30\n",
    "\n",
    "training = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=EPOCHS, \n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0479866193202694, 1.5062817]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD4CAYAAADvsV2wAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAbI0lEQVR4nO3da3AlZ53f8e/v3HTXSLZkPMzFY8CFmTI2dhTAu4Adb5KySQpnq7YoT4W9UNkaXsCCs3FlnX0Rbza1lYRyKBaywZmFiSEh9m7AS7yJd7kavBfjWL7gGXsCDMYXjS/SeK4azehyzj8vuqU5o5F0NNI5o1H371OlOn366Tn9PDT+9XOe7vO0IgIzM8u+wlpXwMzMzg8HvplZTjjwzcxywoFvZpYTDnwzs5worXUFFjIwMBDbtm1b62qYma0bTzzxxMGIGFxqmwsy8Ldt28bw8PBaV8PMbN2Q9GKjbTykY2aWEw58M7OccOCbmeWEA9/MLCcc+GZmOeHANzPLCQe+mVlOZCrwP/fdn/KDn4ytdTXMzC5ImQr8//KDn/GDHzvwzcwWkqnA72kvc/zU9FpXw8zsgpSxwC9x/NTMWlfDzOyClL3An3QP38xsIRkL/LJ7+GZmi8hU4He3lxh34JuZLShTgd/bXuKYA9/MbEGZCnzfpWNmtrhsBX5bicmZGlMztbWuipnZBSdbgd+ePMDLvXwzs7M1DHxJuyWNStq7SPk/lfSMpD2S/lbSNXVlN0v6saT9ku5sZsUX0t1eBmB80uP4ZmbzLaeHfy9w8xLlPwduiIh3Av8W2AUgqQj8EXALsB3YIWn7qmrbwOkevgPfzGy+hoEfEY8Ah5Yo/9uIOJy+/SGwOV1+N7A/Ip6PiCngfuDWVdZ3SbOBf8xDOmZmZ2n2GP4/A/4iXd4EvFxXNpKuW5CknZKGJQ2Pja1sArTedEjHPXwzs7M1LfAl/T2SwP+dlfz7iNgVEUMRMTQ4OLiiOnhIx8xscaVmfIikq4EvArdExBvp6gPAlrrNNqfrWqa7LWnOuId0zMzOsuoevqStwAPAr0bET+qKHgeukHS5pApwG/Dgave3lB4P6ZiZLaphD1/SfcCNwICkEeAuoAwQEfcA/xq4GPjPkgBm0qGZGUmfAL4JFIHdEfFsS1qRqpQKtJUKHPdtmWZmZ2kY+BGxo0H5bwK/uUjZQ8BDK6vaynh6BTOzhWXql7bgCdTMzBaTucD3U6/MzBaWucBP5sT3kI6Z2XyZC/yeNj/1ysxsIdkLfA/pmJktKIOB77t0zMwWksHAL3Fiqkq1FmtdFTOzC0omAx88J76Z2XyZDXwP65iZnSmDge/5dMzMFpLBwPcUyWZmC8lg4M/28D2kY2ZWL3OBPzcnvi/ampmdIXOB3zv3XFsHvplZvcwFvod0zMwWlrnAby8XKBXki7ZmZvM0DHxJuyWNStq7SPmVkh6VNCnpjnll/1zSs5L2SrpPUnuzKr5EfdP5dNzDNzOrt5we/r3AzUuUHwI+Cdxdv1LSpnT9UERcRfKYw9tWVs1zk0yR7B6+mVm9hoEfEY+QhPpi5aMR8TiwUJe6BHRIKgGdwCsrrei58BTJZmZna9kYfkQcIOn1vwS8ChyNiG8ttr2knZKGJQ2PjY2tat+eItnM7GwtC3xJ/cCtwOXAm4EuSR9ZbPuI2BURQxExNDg4uKp997SXOeYxfDOzM7TyLp2/D/w8IsYiYhp4APiFFu5vTq97+GZmZ2ll4L8EvFdSpyQBvwTsa+H+5vS0l/xLWzOzeUqNNpB0H3AjMCBpBLgLKANExD2SLgWGgV6gJul2YHtEPCbpa8CTwAzwFLCrJa2YpzsN/IggOdeYmVnDwI+IHQ3KXwM2L1J2F8kJ4rzqaS9TrQUTU1W62ho20cwsFzL3S1vwFMlmZgvJaOB7Ph0zs/kyGvhpD98Xbs3M5mQz8Ns8pGNmNl82A99DOmZmZ8lo4LuHb2Y2X8YD3z18M7NZmQz8rkoJyT18M7N6mQz8QkF0VzyfjplZvUwGPniKZDOz+TIc+GWP4ZuZ1clw4LuHb2ZWL9uBP+kevpnZrMwGfnd72Q8yNzOrk9nA95COmdmZHPhmZjnRMPAl7ZY0KmnvIuVXSnpU0qSkO+aV9Un6mqT/J2mfpOubVfFGetvLTFVrnJqunq9dmpld0JbTw78XuHmJ8kPAJ4G7Fyj7Q+AvI+JK4BrO0zNtwfPpmJnN1zDwI+IRklBfrHw0Ih4HzrglRtIG4APAl9LtpiLiyOqqu3yzge+HmZuZJVo5hn85MAb8V0lPSfqipK7FNpa0U9KwpOGxsbFV77y7zVMkm5nVa2Xgl4DrgC9ExLXACeDOxTaOiF0RMRQRQ4ODg6veuYd0zMzO1MrAHwFGIuKx9P3XSE4A54WnSDYzO1PLAj8iXgNelvT2dNUvAc+1an/z9aZPvTrmHr6ZGZAMuyxJ0n3AjcCApBHgLqAMEBH3SLoUGAZ6gZqk24HtEXEM+C3gq5IqwPPAR1vSigXMXbR14JuZAcsI/IjY0aD8NWDzImVPA0Mrq9rqdPlB5mZmZ8jsL23LxQId5aLH8M3MUpkNfPD0CmZm9bIf+J4i2cwMyHzgl93DNzNLZTzwPaRjZjYrB4HvIR0zM8h64Ld5SMfMbFa2A99DOmZmczIe+GVOTleZqdbWuipmZmsu04Hf7TnxzczmZDrwPUWymdlpmQ783jTwj/lOHTOzbAd+T/vsU6/cwzczy3jge0jHzGxWxgM/6eGPez4dM7NsB36358Q3M5vTMPAl7ZY0KmnvIuVXSnpU0qSkOxYoL0p6StL/bkaFz4WHdMzMTltOD/9e4OYlyg8BnwTuXqT8U8C+c6tWc7SXi1SKBd+lY2bGMgI/Ih4hCfXFykcj4nHgrFSVtBn4R8AXV1PJ1fD0CmZmiVaP4X8W+JdAw7kNJO2UNCxpeGxsrGkV6Gkv+UHmZma0MPAl/WNgNCKeWM72EbErIoYiYmhwcLBp9ej2FMlmZkBre/i/CHxI0gvA/cBNkv57C/e3IE+RbGaWaFngR8S/iojNEbENuA34XkR8pFX7W4zH8M3MEqVGG0i6D7gRGJA0AtwFlAEi4h5JlwLDQC9Qk3Q7sD0ijrWs1ucgea6th3TMzBoGfkTsaFD+GrC5wTbfB75/LhVrlp72Esc9PbKZWbZ/aQvpXTqTM9RqsdZVMTNbU7kI/Ag4MeVevpnlWw4C31Mkm5lBLgLf8+mYmUEuAt9TJJuZQQ4Cf3aK5GPu4ZtZzmU+8Hs9pGNmBuQg8E9ftPWQjpnlWw4C3z18MzPIQeB3VooUC/IUyWaWe5kPfEl0t3mKZDOzzAc+kAa+e/hmlm+5CPye9pJvyzSz3MtF4Pd6imQzs3wE/uyMmWZmeZabwPcYvpnlXcPAl7Rb0qikvYuUXynpUUmTku6oW79F0sOSnpP0rKRPNbPi58IPMjczW14P/17g5iXKDwGfBO6et34G+BcRsR14L/BxSdtXUsnVSh5zOEOEH4JiZvnVMPAj4hGSUF+sfDQiHgem561/NSKeTJePA/uATaur7sr0tJeYqQWnpmtrsXszswvCeRnDl7QNuBZ4bIltdkoaljQ8NjbW1P17Ph0zs/MQ+JK6ga8Dt0fEscW2i4hdETEUEUODg4NNrcPcjJm+U8fMcqylgS+pTBL2X42IB1q5r6XMzonvO3XMLM9aFviSBHwJ2BcRn2nVfpbDQzpmZlBqtIGk+4AbgQFJI8BdQBkgIu6RdCkwDPQCNUm3A9uBq4FfBfZIejr9uN+NiIea3ooGPEWymdkyAj8idjQofw3YvEDRXwNaYb2a6nTgu4dvZvmVk1/azg7puIdvZvmVi8D3RVszs5wEfrEguipFB76Z5VouAh9mp1fwGL6Z5VeOAt8zZppZvuUq8D0nvpnlWY4C30M6ZpZvuQn8bg/pmFnO5Sbwe/0gczPLudwEvod0zCzv8hP4bSUmZ2pMzfghKGaWT/kJ/HQ+Hd+pY2Z5lZvA7/YUyWaWc7kJfE+RbGZ5l7vAP+YevpnlVG4Cvzcd0hl3D9/Mcqph4EvaLWlU0t5Fyq+U9KikSUl3zCu7WdKPJe2XdGezKr0SHtIxs7xbTg//XuDmJcoPAZ8E7q5fKakI/BFwC8kjD3dI2r6yaq7e6TnxPaRjZvnUMPAj4hGSUF+sfDQiHgfmJ+m7gf0R8XxETAH3A7euprKr4ademVnetXIMfxPwct37kXTdgiTtlDQsaXhsbKzplamUCrSVChz3ffhmllMXzEXbiNgVEUMRMTQ4ONiSfSTTKzjwzSyfWhn4B4Atde83p+vWTG97yWP4ZpZbrQz8x4ErJF0uqQLcBjzYwv015CmSzSzPSo02kHQfcCMwIGkEuAsoA0TEPZIuBYaBXqAm6XZge0Qck/QJ4JtAEdgdEc+2phnL0+MevpnlWMPAj4gdDcpfIxmuWajsIeChlVWt+Xrayowem1zrapiZrYkL5qLt+eAHmZtZnuUs8MueHtnMcitngV9ifHKGai3WuipmZudd7gIf/BAUM8unXAa+79QxszzKWeB7Ph0zy6+cBb6HdMwsv3IW+H6urZnlV64C//Sc+O7hm1n+5Crwe+eea+vAN7P8yVXge0jHzPIsV4HfXi5QKsgPMjezXMpV4EvyfDpmllu5CnyYnRPfQzpmlj+5C/yeNj/m0MzyKX+B7yEdM8upZQW+pN2SRiXtXaRckj4nab+kZyRdV1f2aUnPStqXbqNmVX4letrLHPcvbc0sh5bbw78XuHmJ8luAK9K/ncAXACT9AvCLwNXAVcDfBW5YYV2bwg8yN7O8WlbgR8QjwKElNrkV+Eokfgj0SdoIBNAOVIA2kmfhvr66Kq+OH2RuZnnVrDH8TcDLde9HgE0R8SjwMPBq+vfNiNi30AdI2ilpWNLw2NhYk6p1ttmHoET4IShmli8tvWgr6W3AO0gecr4JuEnS+xfaNiJ2RcRQRAwNDg62rE497WWqtWBiqtqyfZiZXYiaFfgHgC117zen634Z+GFEjEfEOPAXwPVN2ueKeIpkM8urZgX+g8CvpXfrvBc4GhGvAi8BN0gqSSqTXLBdcEjnfPF8OmaWV6XlbCTpPuBGYEDSCHAXyQVYIuIe4CHgg8B+YAL4aPpPvwbcBOwhuYD7lxHx502s/znr8YyZZpZTywr8iNjRoDyAjy+wvgp8bGVVa40ez4lvZjmVw1/aekjHzPIph4GfXrR1D9/Mcia3ge8hHTPLm9wFflelhOQhHTPLn9wFfqEguisl36VjZrmTu8AHT5FsZvmU08Ave0jHzHInp4Ff8tQKZpY7uQ18D+mYWd7kMvC7PaRjZjmUy8B3D9/M8siBb2aWE7kM/Iu7KkxVa/z+nz/H6PFTa10dM7PzYlmzZWbNbe/eyk9fH+fLj77AVx97kY+89zI+dsNbuKSnfa2rZmbWMroQn+06NDQUw8PDLd/PCwdP8Pnv7efPnhqhUirwkfdcxsdueCuDPW0t37eZWTNJeiIihpbcJs+BP+vnB0/w+e/9lG88dYBKqcCvXb+NnR94CwPdDn4zWx+WE/gNx/Al7ZY0KmnvIuWS9DlJ+yU9I+m6urKtkr4laZ+k5yRtO9dGnA+XD3TxmQ+/i+/89g188KqNfPGvnuf9/+Fh/t1D+zg4PrnW1TMza4qGPXxJHwDGga9ExFULlH8Q+C2SRxy+B/jDiHhPWvZ94A8i4tuSuoFaREw0qtT57uHP97OxcT7/3Z/y4I9eoa1U5KYrL2FoWz9Dl13EOzb2UCrm8lq3mV3AltPDb3jRNiIeadAzv5XkZBDADyX1SdoI9AOliPh2+jnjy675GnvrYDefve1aPnHTFex65Gf8zf43+D97XgWgs1LkXVv6GLqsn6FtF3Ht1r65p2iZmV3ImnGXzibg5br3I+m6zcARSQ8AlwPfAe5Mn3N7Fkk7gZ0AW7dubUK1Vu9tl3Tz6V+5BoBXjpxk+MXDPPHCIR5/4TD/6eH91AIKgrdf2svQZf1cvXkDl/S2M9BdYbC7jf6uCmV/GzCzC0Qrb8ssAe8HrgVeAv4E+A3gSwttHBG7gF2QDOm0sF4r8ua+Dj7U18GHrnkzkDxA5emXjzD8wmGGXzzE158c4b/98MWz/l1/Z5mB7jYu7q4w0N3GQHcbgz1tXNRVob+zwkVdFS7qKtPfWaGvs0KxoGXXaWqmxonJGcbTvzf3dbChw982zGxhzQj8A8CWuveb03Ul4OmIeB5A0jeA97JI4K83Pe1l3n/FIO+/YhCAmWqNlw+f5OD4JAePT3LwxFTyOj7JG+NTHByfZO+Bo7wxPsXxRWbqlGBDRzk5CXRW6O+q0FkpcmJyhuOnTgf7eLo8OVM76zPedkk3127p49qt/Vx3WR9XXNJzTicRM8uuZgT+g8AnJN1PctH2aES8KmkU6JM0GBFjwE3A2l2JbbFSscDlA11cPtDVcNtT01UOT0xx6MQUh09Mc2hiisMn0vcTp19HDp9kYmqGrkqJ7vYSl/a2091eorsted/TNrtcpqNc5OcHx3nypSN8Z9/r/M8nRgDoqhS5Zksf127t47qt/bxrSx8XN+F202otGD1+ipHDJxk5PMGp6RrbN/Zy5cYe2krFVX++mTVfw8CXdB9wIzAgaQS4CygDRMQ9wEMkd+jsByaAj6ZlVUl3AN+VJOAJ4I9b0IZ1p71cZOOGDjZu6GjJ50cEL74xwVMvH+apl47w5EuHuecHz1OtJSNll/a2J0NKXWX6Oips6CzT35ks93WW6eus0N9ZprejzJGJaUYOT8wF+4EjJxk5fJJXjpxkunr2yFu5KN5+aQ/v3NTH1Zs38M5NG3j7pT2+lmF2AfAPr3Li5FSVPQeO8tRLh/nJ6+McPTnF4YlpjkxMcWRimiMnp+dOCIu5pKeNTf0dbO7vZHN/R/rXyaa+DirFAs++cpRnDhxlz8hRnhk5Mvfc4EqpwDs29nL1puQE0NtRBoIImN1jsnzmuq5Kkas2beBNvZ7ywqwR/9LWli0iGJ+cScJ/YprDE1McPTnNho4ym/s7eHNfB+3l5Q/VRAQvHZrgmZGj7DmQnAD2Hji2oieNXdLTxtWbk28MyV8fF3VVzvlzzLLMgW8XlFotOQlMTCV35krpH0LpdWWl60EcPTmVnDBGjvKjkSM8f/AEs/933dTXwTVbNswNHb2pt52utiKdlRJdlaJ/HGe505QfXpk1S6Egti3jona9v3PZRXPLx09Ns/fAMfYcOMKP0hPBQ3teW/DftZUKdLeV6Gwr0lUp0dVWorNSpL1cZKF7lrTAyoJEoSCKEsWCKEgUC9QtJ6+VUoHOSrKfzrZist/0xNPVVqo7EZUoFZV+bvr5EgWBFqpAqlYLqhFUa8nfTO30crUWFApQKhQoFkSpoDNel/rciKAWMFOrUauR7KOa7KtYEJVigXKx8ecs9tnVWjBVrTE9k7wGMXdyT07smjvBi2Rlfdly1CKIWvJajaAWQS19X78cJL+ZmT2mBUFRSbvmr589tkq3mS3LAge+rRs97WWuf+vFXP/Wi+fWHT4xxbOvHOONE5NMTFU5MTnDickqE1MznJhKlk9MzjAxVeX4qRnGji9vbqSIuhCpzb4mdyfVr6vWgulqjVPTZ98iey40GzrpKyT7n6kFq/kSXqw7AQjm2jFTq9Hgks0ZdSsXC3MngHKxkLwvFZBgui7Up2dqTFZrTFdrq6r3hagg5k5+ycligWtP897PH0GZPckly+lrerIDGOxu42/uvKllbXDg27rW31XhfVcMrHU1qNaCiankxDI+OcPEZJUTUzNMTM0wPlllYnKGE1NVqrUa1dleZ8QZy7VgrpdKMBfUhXmvxUKBoqBYLFCUkhPTXM+/lrxWT38TmF1fi9MngNlvLWf8zX2jgWrMBnkS3lPV5MQ2+zc1k7yvRtCWngDKJVEpFimXVLfu9IliNiGTIEzCcG6Z0+G43BNFEGd8SyoUNPe+WCDtvSfLwOneft3/zrU4/W2kFsm3qdn1tfqTfV1ZNf2MWi3SYck0xOuGJ+u/ucwG++zJob6NccZy0FVpbSQ78M2aoFgQPe1letrLvGmtK2O2CF/ZMjPLCQe+mVlOOPDNzHLCgW9mlhMOfDOznHDgm5nlhAPfzCwnHPhmZjlxQU6eJmkMOPt5gcszABxsYnXWWtbaA9lrU9baA9lrU9baA2e36bKIGFzqH1yQgb8akoYbzRi3nmStPZC9NmWtPZC9NmWtPbCyNnlIx8wsJxz4ZmY5kcXA37XWFWiyrLUHstemrLUHstemrLUHVtCmzI3hm5nZwrLYwzczswU48M3MciIzgS/pZkk/lrRf0p1rXZ9mkPSCpD2Snpa0Lp/qLmm3pFFJe+vWXSTp25J+mr72r2Udz8Ui7fk9SQfS4/S0pA+uZR3PhaQtkh6W9JykZyV9Kl2/no/RYm1al8dJUruk/yvpR2l7/k26/nJJj6WZ9yeSKg0/Kwtj+JKKwE+AfwCMAI8DOyLiuTWt2CpJegEYioh1+4MRSR8AxoGvRMRV6bpPA4ci4t+nJ+f+iPidtaznci3Snt8DxiPi7rWs20pI2ghsjIgnJfUATwD/BPgN1u8xWqxNH2YdHiclT3TviohxSWXgr4FPAb8NPBAR90u6B/hRRHxhqc/KSg//3cD+iHg+IqaA+4Fb17hOBkTEI8CheatvBb6cLn+Z5D/GdWGR9qxbEfFqRDyZLh8H9gGbWN/HaLE2rUuRGE/fltO/AG4CvpauX9YxykrgbwJerns/wjo+wHUC+JakJyTtXOvKNNGbIuLVdPk1yMRjYD8h6Zl0yGfdDH/Uk7QNuBZ4jIwco3ltgnV6nCQVJT0NjALfBn4GHImImXSTZWVeVgI/q94XEdcBtwAfT4cTMiWSMcX1Pq74BeCtwLuAV4H/uLbVOXeSuoGvA7dHxLH6svV6jBZo07o9ThFRjYh3AZtJRjSuXMnnZCXwDwBb6t5vTtetaxFxIH0dBf6M5EBnwevpOOvseOvoGtdnVSLi9fQ/yBrwx6yz45SOC38d+GpEPJCuXtfHaKE2rffjBBARR4CHgeuBPkmltGhZmZeVwH8cuCK9al0BbgMeXOM6rYqkrvSCE5K6gH8I7F36X60bDwK/ni7/OvC/1rAuqzYbjKlfZh0dp/SC4JeAfRHxmbqidXuMFmvTej1OkgYl9aXLHSQ3p+wjCf5fSTdb1jHKxF06AOktVp8FisDuiPiDNa7Sqkh6C0mvHqAE/I/12CZJ9wE3kkzl+jpwF/AN4E+BrSTTYH84ItbFhdBF2nMjyTBBAC8AH6sb/76gSXof8FfAHqCWrv5dkjHv9XqMFmvTDtbhcZJ0NclF2SJJJ/1PI+L304y4H7gIeAr4SERMLvlZWQl8MzNbWlaGdMzMrAEHvplZTjjwzcxywoFvZpYTDnwzs5xw4JuZ5YQD38wsJ/4/M1uuNsPCHlIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "model_hist = training.history\n",
    "plt.plot(model_hist['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.4460096],\n",
       "       [2.4460096],\n",
       "       [2.4460096],\n",
       "       ...,\n",
       "       [2.4460096],\n",
       "       [2.4460096],\n",
       "       [2.4460096]], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Spielen mit trainiertem neuronalen Netz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_4 to have 4 dimensions, but got array with shape (84, 84, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e135e475c547>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mEpisode_Reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mEpisode_Reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    381\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    557\u001b[0m   adapter = adapter_cls(x, y, batch_size=batch_size, steps=steps,\n\u001b[1;32m    558\u001b[0m                         \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2465\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2466\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2467\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2468\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2469\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    561\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_4 to have 4 dimensions, but got array with shape (84, 84, 1)"
     ]
    }
   ],
   "source": [
    "EPISODES = 5\n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    Episode_Reward = 0\n",
    "    while not done:\n",
    "        action = model.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        Episode_Reward += reward\n",
    "        \n",
    "        if done:\n",
    "            print(\"Episode:\", episode, \"\\tReward:\", Episode_Reward)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
